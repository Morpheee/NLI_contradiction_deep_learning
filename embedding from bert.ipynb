{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Create new `pandas` methods which use `tqdm` progress\n",
    "# (can use tqdm_gui, optional kwargs, etc.)\n",
    "tqdm.pandas()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "load data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../data/translated_data_train.csv\")\n",
    "df_test = pd.read_csv(\"../data/translated_data_test.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "load tokenizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# https://albertauyeung.github.io/2020/06/19/bert-tokenization.html/\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "              id                                            premise  \\\n2049  e92393740b  ผู้แสวงบุญจะซื้อเค้กน้ำผึ้งให้งูตัวนี้และวางเค...   \n\n                                             hypothesis lang_abv language  \\\n2049  งูจะกินผู้แสวงบุญซึ่งพยายามเข้าถึงพระวิหารเท่า...       th     Thai   \n\n      label  \n2049      2  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>premise</th>\n      <th>hypothesis</th>\n      <th>lang_abv</th>\n      <th>language</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2049</th>\n      <td>e92393740b</td>\n      <td>ผู้แสวงบุญจะซื้อเค้กน้ำผึ้งให้งูตัวนี้และวางเค...</td>\n      <td>งูจะกินผู้แสวงบุญซึ่งพยายามเข้าถึงพระวิหารเท่า...</td>\n      <td>th</td>\n      <td>Thai</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[df_train[\"id\"] == \"e92393740b\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "concatenate and add special tokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "df_train[\"concatenated\"] = \"[CLS]\" + df_train[\"premise\"] + \"[SEP]\" + df_train[\"hypothesis\"]\n",
    "df_test[\"concatenated\"] = \"[CLS]\" + df_test[\"premise\"] + \"[SEP]\" + df_test[\"hypothesis\"]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "tokenize (wordPiece)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12120/12120 [00:06<00:00, 2016.75it/s]\n",
      "100%|██████████| 5195/5195 [00:02<00:00, 2038.83it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train[\"tokens\"] = df_train[\"concatenated\"].progress_apply(tokenizer.tokenize)\n",
    "df_test[\"tokens\"] = df_test[\"concatenated\"].progress_apply(tokenizer.tokenize)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "               id                                            premise  \\\n4758   e010c79cbd                                          I am not.   \n4638   b65dc1311f                                          Waterloo.   \n1919   4527529e3d                                          'Go now.'   \n8557   05e75f22ac                                       'Of course.'   \n7058   f7a6243cc7                                           'I see.'   \n...           ...                                                ...   \n12042  1934ec8b05  and i look back on that and i bought shoes i w...   \n6931   2b8b1bd7e9  and i look back on that and i bought shoes i w...   \n5681   1a087ea88f  yes they would they just wouldn't be able to o...   \n4843   373902d224  yes they would they just wouldn't be able to o...   \n10090  d35aed4384  yes they would they just wouldn't be able to o...   \n\n                                              hypothesis lang_abv language  \\\n4758                                               I am.       en  English   \n4638                                              D-Day.       en  English   \n1919                                              Stay.        en  English   \n8557                                                Yes.       en  English   \n7058                                           I saw it.       en  English   \n...                                                  ...      ...      ...   \n12042  I am envious of all my debt-free churchgoing f...       en  English   \n6931   My friends should look towards me as a model o...       en  English   \n5681               I am glad our generation has no debt.       en  English   \n4843   Life will be great for subsequent generations ...       en  English   \n10090  Society would be perfect and there would be no...       en  English   \n\n       label                                       concatenated  \\\n4758       2                           [CLS]I am not.[SEP]I am.   \n4638       2                          [CLS]Waterloo.[SEP]D-Day.   \n1919       2                          [CLS]'Go now.'[SEP]Stay.    \n8557       0                         [CLS]'Of course.'[SEP]Yes.   \n7058       0                        [CLS]'I see.'[SEP]I saw it.   \n...      ...                                                ...   \n12042      1  [CLS]and i look back on that and i bought shoe...   \n6931       1  [CLS]and i look back on that and i bought shoe...   \n5681       2  [CLS]yes they would they just wouldn't be able...   \n4843       1  [CLS]yes they would they just wouldn't be able...   \n10090      1  [CLS]yes they would they just wouldn't be able...   \n\n                                                  tokens   len  \n4758             [[CLS], i, am, not, ., [SEP], i, am, .]    24  \n4638           [[CLS], waterloo, ., [SEP], d, -, day, .]    25  \n1919           [[CLS], ', go, now, ., ', [SEP], stay, .]    25  \n8557         [[CLS], ', of, course, ., ', [SEP], yes, .]    26  \n7058      [[CLS], ', i, see, ., ', [SEP], i, saw, it, .]    27  \n...                                                  ...   ...  \n12042  [[CLS], and, i, look, back, on, that, and, i, ...   955  \n6931   [[CLS], and, i, look, back, on, that, and, i, ...   963  \n5681   [[CLS], yes, they, would, they, just, wouldn, ...  1014  \n4843   [[CLS], yes, they, would, they, just, wouldn, ...  1059  \n10090  [[CLS], yes, they, would, they, just, wouldn, ...  1076  \n\n[6870 rows x 9 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>premise</th>\n      <th>hypothesis</th>\n      <th>lang_abv</th>\n      <th>language</th>\n      <th>label</th>\n      <th>concatenated</th>\n      <th>tokens</th>\n      <th>len</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>4758</th>\n      <td>e010c79cbd</td>\n      <td>I am not.</td>\n      <td>I am.</td>\n      <td>en</td>\n      <td>English</td>\n      <td>2</td>\n      <td>[CLS]I am not.[SEP]I am.</td>\n      <td>[[CLS], i, am, not, ., [SEP], i, am, .]</td>\n      <td>24</td>\n    </tr>\n    <tr>\n      <th>4638</th>\n      <td>b65dc1311f</td>\n      <td>Waterloo.</td>\n      <td>D-Day.</td>\n      <td>en</td>\n      <td>English</td>\n      <td>2</td>\n      <td>[CLS]Waterloo.[SEP]D-Day.</td>\n      <td>[[CLS], waterloo, ., [SEP], d, -, day, .]</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>1919</th>\n      <td>4527529e3d</td>\n      <td>'Go now.'</td>\n      <td>Stay.</td>\n      <td>en</td>\n      <td>English</td>\n      <td>2</td>\n      <td>[CLS]'Go now.'[SEP]Stay.</td>\n      <td>[[CLS], ', go, now, ., ', [SEP], stay, .]</td>\n      <td>25</td>\n    </tr>\n    <tr>\n      <th>8557</th>\n      <td>05e75f22ac</td>\n      <td>'Of course.'</td>\n      <td>Yes.</td>\n      <td>en</td>\n      <td>English</td>\n      <td>0</td>\n      <td>[CLS]'Of course.'[SEP]Yes.</td>\n      <td>[[CLS], ', of, course, ., ', [SEP], yes, .]</td>\n      <td>26</td>\n    </tr>\n    <tr>\n      <th>7058</th>\n      <td>f7a6243cc7</td>\n      <td>'I see.'</td>\n      <td>I saw it.</td>\n      <td>en</td>\n      <td>English</td>\n      <td>0</td>\n      <td>[CLS]'I see.'[SEP]I saw it.</td>\n      <td>[[CLS], ', i, see, ., ', [SEP], i, saw, it, .]</td>\n      <td>27</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>12042</th>\n      <td>1934ec8b05</td>\n      <td>and i look back on that and i bought shoes i w...</td>\n      <td>I am envious of all my debt-free churchgoing f...</td>\n      <td>en</td>\n      <td>English</td>\n      <td>1</td>\n      <td>[CLS]and i look back on that and i bought shoe...</td>\n      <td>[[CLS], and, i, look, back, on, that, and, i, ...</td>\n      <td>955</td>\n    </tr>\n    <tr>\n      <th>6931</th>\n      <td>2b8b1bd7e9</td>\n      <td>and i look back on that and i bought shoes i w...</td>\n      <td>My friends should look towards me as a model o...</td>\n      <td>en</td>\n      <td>English</td>\n      <td>1</td>\n      <td>[CLS]and i look back on that and i bought shoe...</td>\n      <td>[[CLS], and, i, look, back, on, that, and, i, ...</td>\n      <td>963</td>\n    </tr>\n    <tr>\n      <th>5681</th>\n      <td>1a087ea88f</td>\n      <td>yes they would they just wouldn't be able to o...</td>\n      <td>I am glad our generation has no debt.</td>\n      <td>en</td>\n      <td>English</td>\n      <td>2</td>\n      <td>[CLS]yes they would they just wouldn't be able...</td>\n      <td>[[CLS], yes, they, would, they, just, wouldn, ...</td>\n      <td>1014</td>\n    </tr>\n    <tr>\n      <th>4843</th>\n      <td>373902d224</td>\n      <td>yes they would they just wouldn't be able to o...</td>\n      <td>Life will be great for subsequent generations ...</td>\n      <td>en</td>\n      <td>English</td>\n      <td>1</td>\n      <td>[CLS]yes they would they just wouldn't be able...</td>\n      <td>[[CLS], yes, they, would, they, just, wouldn, ...</td>\n      <td>1059</td>\n    </tr>\n    <tr>\n      <th>10090</th>\n      <td>d35aed4384</td>\n      <td>yes they would they just wouldn't be able to o...</td>\n      <td>Society would be perfect and there would be no...</td>\n      <td>en</td>\n      <td>English</td>\n      <td>1</td>\n      <td>[CLS]yes they would they just wouldn't be able...</td>\n      <td>[[CLS], yes, they, would, they, just, wouldn, ...</td>\n      <td>1076</td>\n    </tr>\n  </tbody>\n</table>\n<p>6870 rows × 9 columns</p>\n</div>"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(df_train[df_train[\"language\"] == \"English\"])\n",
    "df[\"len\"] = df[\"concatenated\"].apply(len)\n",
    "df.sort_values(by=\"len\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "               id                                            premise  \\\n1399   4177df956e                                    Bork shuddered.   \n10876  231fa739a0                                       Bauerstein.\"   \n8418   7cbc3d1337                                     said San'doro.   \n9392   ff304862ca                                    God i'm envious   \n7201   5aeb875f88                                   Then he sobered.   \n...           ...                                                ...   \n11062  6c8e685b32  8 A stoichiometry of 1.03 is typical when the ...   \n3700   cd1b130fa5  8 A stoichiometry of 1.03 is typical when the ...   \n8425   b05536587b  But they also don't seem to mind when the tran...   \n4819   629bd6c484  But they also don't seem to mind when the tran...   \n11390  62e3904e06  But they also don't seem to mind when the tran...   \n\n                                              hypothesis lang_abv language  \\\n1399                                      Bork shivered.       en  English   \n10876                                  Doctor Bauerstein       en  English   \n8418                                    San'doro spoke.        en  English   \n9392                                  Lord, I'm envious.       en  English   \n7201                                  He had sobered up.       en  English   \n...                                                  ...      ...      ...   \n11062  A stoichiometry of 1.03 is typical when the FG...       en  English   \n3700   A stoichiometry of 1.03 is typical when the FG...       en  English   \n8425            A Zen temple rock garden is a zen place.       en  English   \n4819   A Zen temple rock garden is a a place for lots...       en  English   \n11390          A temple garden doesnt allow electronics.       en  English   \n\n       label                                       concatenated  \\\n1399       0            [CLS]Bork shuddered.[SEP]Bork shivered.   \n10876      1            [CLS]Bauerstein.\"[SEP]Doctor Bauerstein   \n8418       0          [CLS] said San'doro.[SEP]San'doro spoke.    \n9392       0        [CLS]God i'm envious[SEP]Lord, I'm envious.   \n7201       0       [CLS]Then he sobered.[SEP]He had sobered up.   \n...      ...                                                ...   \n11062      0  [CLS]8 A stoichiometry of 1.03 is typical when...   \n3700       2  [CLS]8 A stoichiometry of 1.03 is typical when...   \n8425       0  [CLS]But they also don't seem to mind when the...   \n4819       1  [CLS]But they also don't seem to mind when the...   \n11390      2  [CLS]But they also don't seem to mind when the...   \n\n                                                  tokens  len  sum  \n1399   [[CLS], bo, ##rk, shuddered, ., [SEP], bo, ##r...   39    2  \n10876  [[CLS], bauer, ##stein, ., \", [SEP], doctor, b...   39    2  \n8418   [[CLS], said, san, ', do, ##ro, ., [SEP], san,...   41    2  \n9392   [[CLS], god, i, ', m, en, ##vious, [SEP], lord...   43    2  \n7201   [[CLS], then, he, sober, ##ed, ., [SEP], he, h...   44    2  \n...                                                  ...  ...  ...  \n11062  [[CLS], 8, a, st, ##oic, ##hi, ##ome, ##try, o...  267   20  \n3700   [[CLS], 8, a, st, ##oic, ##hi, ##ome, ##try, o...  271   20  \n8425   [[CLS], but, they, also, don, ', t, seem, to, ...  560   20  \n4819   [[CLS], but, they, also, don, ', t, seem, to, ...  601   20  \n11390  [[CLS], but, they, also, don, ', t, seem, to, ...  561   21  \n\n[2860 rows x 10 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>premise</th>\n      <th>hypothesis</th>\n      <th>lang_abv</th>\n      <th>language</th>\n      <th>label</th>\n      <th>concatenated</th>\n      <th>tokens</th>\n      <th>len</th>\n      <th>sum</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1399</th>\n      <td>4177df956e</td>\n      <td>Bork shuddered.</td>\n      <td>Bork shivered.</td>\n      <td>en</td>\n      <td>English</td>\n      <td>0</td>\n      <td>[CLS]Bork shuddered.[SEP]Bork shivered.</td>\n      <td>[[CLS], bo, ##rk, shuddered, ., [SEP], bo, ##r...</td>\n      <td>39</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>10876</th>\n      <td>231fa739a0</td>\n      <td>Bauerstein.\"</td>\n      <td>Doctor Bauerstein</td>\n      <td>en</td>\n      <td>English</td>\n      <td>1</td>\n      <td>[CLS]Bauerstein.\"[SEP]Doctor Bauerstein</td>\n      <td>[[CLS], bauer, ##stein, ., \", [SEP], doctor, b...</td>\n      <td>39</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>8418</th>\n      <td>7cbc3d1337</td>\n      <td>said San'doro.</td>\n      <td>San'doro spoke.</td>\n      <td>en</td>\n      <td>English</td>\n      <td>0</td>\n      <td>[CLS] said San'doro.[SEP]San'doro spoke.</td>\n      <td>[[CLS], said, san, ', do, ##ro, ., [SEP], san,...</td>\n      <td>41</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>9392</th>\n      <td>ff304862ca</td>\n      <td>God i'm envious</td>\n      <td>Lord, I'm envious.</td>\n      <td>en</td>\n      <td>English</td>\n      <td>0</td>\n      <td>[CLS]God i'm envious[SEP]Lord, I'm envious.</td>\n      <td>[[CLS], god, i, ', m, en, ##vious, [SEP], lord...</td>\n      <td>43</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>7201</th>\n      <td>5aeb875f88</td>\n      <td>Then he sobered.</td>\n      <td>He had sobered up.</td>\n      <td>en</td>\n      <td>English</td>\n      <td>0</td>\n      <td>[CLS]Then he sobered.[SEP]He had sobered up.</td>\n      <td>[[CLS], then, he, sober, ##ed, ., [SEP], he, h...</td>\n      <td>44</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>11062</th>\n      <td>6c8e685b32</td>\n      <td>8 A stoichiometry of 1.03 is typical when the ...</td>\n      <td>A stoichiometry of 1.03 is typical when the FG...</td>\n      <td>en</td>\n      <td>English</td>\n      <td>0</td>\n      <td>[CLS]8 A stoichiometry of 1.03 is typical when...</td>\n      <td>[[CLS], 8, a, st, ##oic, ##hi, ##ome, ##try, o...</td>\n      <td>267</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>3700</th>\n      <td>cd1b130fa5</td>\n      <td>8 A stoichiometry of 1.03 is typical when the ...</td>\n      <td>A stoichiometry of 1.03 is typical when the FG...</td>\n      <td>en</td>\n      <td>English</td>\n      <td>2</td>\n      <td>[CLS]8 A stoichiometry of 1.03 is typical when...</td>\n      <td>[[CLS], 8, a, st, ##oic, ##hi, ##ome, ##try, o...</td>\n      <td>271</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>8425</th>\n      <td>b05536587b</td>\n      <td>But they also don't seem to mind when the tran...</td>\n      <td>A Zen temple rock garden is a zen place.</td>\n      <td>en</td>\n      <td>English</td>\n      <td>0</td>\n      <td>[CLS]But they also don't seem to mind when the...</td>\n      <td>[[CLS], but, they, also, don, ', t, seem, to, ...</td>\n      <td>560</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>4819</th>\n      <td>629bd6c484</td>\n      <td>But they also don't seem to mind when the tran...</td>\n      <td>A Zen temple rock garden is a a place for lots...</td>\n      <td>en</td>\n      <td>English</td>\n      <td>1</td>\n      <td>[CLS]But they also don't seem to mind when the...</td>\n      <td>[[CLS], but, they, also, don, ', t, seem, to, ...</td>\n      <td>601</td>\n      <td>20</td>\n    </tr>\n    <tr>\n      <th>11390</th>\n      <td>62e3904e06</td>\n      <td>But they also don't seem to mind when the tran...</td>\n      <td>A temple garden doesnt allow electronics.</td>\n      <td>en</td>\n      <td>English</td>\n      <td>2</td>\n      <td>[CLS]But they also don't seem to mind when the...</td>\n      <td>[[CLS], but, they, also, don, ', t, seem, to, ...</td>\n      <td>561</td>\n      <td>21</td>\n    </tr>\n  </tbody>\n</table>\n<p>2860 rows × 10 columns</p>\n</div>"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"sum\"] = df[\"tokens\"].apply(lambda x : np.sum([(\"##\" in tok) for tok in x]))\n",
    "df[df[\"sum\"] > 1].sort_values(by=[\"sum\",'len'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "get maximum tokens list length"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max length = 308\n"
     ]
    }
   ],
   "source": [
    "max_length = max(df_train[\"tokens\"].apply(len).max(), df_test[\"tokens\"].apply(len).max())\n",
    "print(\"max length =\", max_length)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "add padding to make everything uniform length_wise"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def add_padding(tokens):\n",
    "    tokens = tokens + [\"[PAD]\"] * (max_length - len(tokens))\n",
    "    return tokens"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12120/12120 [00:00<00:00, 122829.82it/s]\n",
      "100%|██████████| 5195/5195 [00:00<00:00, 92044.85it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train[\"tokens_pad\"] = df_train[\"tokens\"].progress_apply(add_padding)\n",
    "df_test[\"tokens_pad\"] = df_test[\"tokens\"].progress_apply(add_padding)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "get tokens_index"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12120/12120 [00:02<00:00, 4059.51it/s]\n",
      "100%|██████████| 5195/5195 [00:01<00:00, 4009.79it/s]\n"
     ]
    }
   ],
   "source": [
    "# https://medium.com/@dhartidhami/understanding-bert-word-embeddings-7dc4d2ea54ca\n",
    "# Map the token strings to their vocabulary indeces.\n",
    "df_train[\"indexed_tokens\"] = df_train[\"tokens_pad\"].progress_apply(tokenizer.convert_tokens_to_ids)\n",
    "df_test[\"indexed_tokens\"] = df_test[\"tokens_pad\"].progress_apply(tokenizer.convert_tokens_to_ids)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "get segments_ids (0 for first sentence and `[SEP]`, 1 for the rest)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def get_segments_ids(tokens: list):\n",
    "    segments_ids = [0] * (tokens.index(\"[SEP]\") + 1) + [1] * (len(tokens) - (tokens.index(\"[SEP]\") + 1))\n",
    "    return segments_ids"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12120/12120 [00:00<00:00, 108413.93it/s]\n",
      "100%|██████████| 5195/5195 [00:00<00:00, 17992.12it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train[\"segments_ids\"] = df_train[\"tokens_pad\"].progress_apply(get_segments_ids)\n",
    "df_test[\"segments_ids\"] = df_test[\"tokens_pad\"].progress_apply(get_segments_ids)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "convert to tensors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12120/12120 [00:00<00:00, 34361.04it/s]\n",
      "100%|██████████| 5195/5195 [00:00<00:00, 30860.91it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train[\"tokens_tensor\"] = df_train[\"indexed_tokens\"].progress_apply(lambda x: torch.tensor([x]))\n",
    "df_test[\"tokens_tensor\"] = df_test[\"indexed_tokens\"].progress_apply(lambda x: torch.tensor([x]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12120/12120 [00:00<00:00, 34534.53it/s]\n",
      "100%|██████████| 5195/5195 [00:00<00:00, 30454.08it/s]\n"
     ]
    }
   ],
   "source": [
    "df_train[\"segments_tensor\"] = df_train[\"segments_ids\"].progress_apply(lambda x: torch.tensor([x]))\n",
    "df_test[\"segments_tensor\"] = df_test[\"segments_ids\"].progress_apply(lambda x: torch.tensor([x]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "load model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertModel.from_pretrained(\"bert-base-uncased\", output_hidden_states=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "BertModel(\n  (embeddings): BertEmbeddings(\n    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n    (position_embeddings): Embedding(512, 768)\n    (token_type_embeddings): Embedding(2, 768)\n    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder): BertEncoder(\n    (layer): ModuleList(\n      (0): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (1): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (2): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (3): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (4): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (5): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (6): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (7): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (8): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (9): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (10): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (11): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (pooler): BertPooler(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (activation): Tanh()\n  )\n)"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "get embedding"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12120/12120 [1:06:11<00:00,  3.05it/s]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    df_train[\"sentence_embedding\"] = df_train[[\"tokens_tensor\", \"segments_tensor\"]].progress_apply(\n",
    "        lambda x: torch.mean(model(x[\"tokens_tensor\"], x[\"segments_tensor\"])[2][-2][0], dim=0), axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5195/5195 [32:00<00:00,  2.70it/s]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    df_test[\"sentence_embedding\"] = df_test[[\"tokens_tensor\", \"segments_tensor\"]].progress_apply(\n",
    "        lambda x: torch.mean(model(x[\"tokens_tensor\"], x[\"segments_tensor\"])[2][-2][0], dim=0), axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "df_train[\"sentence_embedding\"] = df_train[\"sentence_embedding\"].apply(lambda x : list(np.array(x)))\n",
    "df_test[\"sentence_embedding\"] = df_test[\"sentence_embedding\"].apply(lambda x : list(np.array(x)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "df_train[[\"id\", \"premise\", \"hypothesis\", \"lang_abv\", \"language\", \"label\", \"sentence_embedding\"]].to_csv(\n",
    "    \"../data/train_embedded_bert.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "df_test[[\"id\", \"premise\", \"hypothesis\", \"lang_abv\", \"language\", \"sentence_embedding\"]].to_csv(\n",
    "    \"../data/test_embedded_bert.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}