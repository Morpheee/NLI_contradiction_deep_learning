{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_bert_experiments.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjpC-OeJ8a7l"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
        "from keras.models import Model\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM,Dense,Dropout,Embedding,CuDNNLSTM,Bidirectional, Reshape, Conv2D, MaxPool2D, Concatenate, Flatten, MaxPool1D\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "jKZdjbcW-ybz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "size_vect = 768\n",
        "\n",
        "def label_to_one_hot(label):\n",
        "  one_hot = [0, 0, 0]\n",
        "  one_hot[label] = 1\n",
        "  return one_hot\n",
        "\n",
        "def prepare_data_bert(data):\n",
        "  \n",
        "  data[\"sentence_embedding\"] = data[\"sentence_embedding\"].apply(lambda x : np.asarray([float(value) for value in x[1:-1].split(\",\")]).astype(np.float32))\n",
        "\n",
        "  x_train, x_test, _, y_test = train_test_split(data['sentence_embedding'], data['label'], test_size=0.3, random_state=42)\n",
        "\n",
        "  x_train_to_return = np.zeros(shape=(len(x_train), 768))\n",
        "  for i in range(len(x_train)):\n",
        "      x_train_to_return[i] = x_train[\"sentence_embedding\"].values[i]\n",
        "\n",
        "  x_test_to_return = np.zeros(shape=(len(x_test), 768))\n",
        "  for i in range(len(x_test)):\n",
        "      x_test_to_return[i] = x_test[\"sentence_embedding\"].values[i]\n",
        "\n",
        "  data[\"yhot\"] = data[\"label\"].apply(label_to_one_hot)\n",
        "  y_train = np.zeros(shape=(len(data), 3))\n",
        "  for i in range(len(data)):\n",
        "      y_train[i] = data[\"yhot\"].values[i]\n",
        "  y_train = y_train.astype(int)\n",
        "\n",
        "  return x_train_to_return, x_test_to_return, y_train, y_test\n"
      ],
      "metadata": {
        "id": "KFc5nq_5-u5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cnn_1(size_vector=768, metric='accuracy'):\n",
        "  filter_sizes = [5,50,3]\n",
        "  num_filters = 10\n",
        "  drop = 0.9\n",
        "  batch_size = 15\n",
        "  epochs = 2\n",
        "\n",
        "  inputs = Input(shape=(size_vector,1), dtype='float32')\n",
        "  conv_0 = Conv1D(num_filters, kernel_size=filter_sizes[0], padding='valid', kernel_initializer='normal', activation='relu')(inputs)\n",
        "  conv_1 = Conv1D(num_filters, kernel_size=filter_sizes[1], padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
        "  conv_2 = Conv1D(num_filters, kernel_size=filter_sizes[2], padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
        "\n",
        "  maxpool_0 = MaxPool1D(pool_size=2, padding='same')(conv_0)\n",
        "  maxpool_1 = MaxPool1D(pool_size=(300 - filter_sizes[1] + 1), padding='valid')(conv_1)\n",
        "  maxpool_2 = MaxPool1D(pool_size=(300 - filter_sizes[2] + 1), padding='valid')(conv_2)\n",
        "\n",
        "  concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\n",
        "  flatten = Flatten()(concatenated_tensor)\n",
        "  dropout = Dropout(drop)(flatten)\n",
        "  # dense = Dense(units=294912, activation='softmax')(dropout)\n",
        "  output = Dense(units=3, activation='softmax')(dropout)\n",
        "  model = Model(inputs=inputs, outputs=output)\n",
        "  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[metric])\n",
        "  return model\n",
        "\n",
        "def cnn_2(size_vector=768, metric='accuracy'):\n",
        "  filter_sizes = [5,50,3]\n",
        "  num_filters = 10\n",
        "  drop = 0.9\n",
        "  batch_size = 15\n",
        "  epochs = 2\n",
        "\n",
        "  inputs = Input(shape=(size_vector,1), dtype='float32')\n",
        "  conv_0 = Conv2D(num_filters, kernel_size=filter_sizes[0], padding='valid', kernel_initializer='normal', activation='relu')(inputs)\n",
        "  conv_1 = Conv2D(num_filters, kernel_size=filter_sizes[1], padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
        "  conv_2 = Conv2D(num_filters, kernel_size=filter_sizes[2], padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
        "\n",
        "  maxpool_0 = MaxPool2D(pool_size=2, padding='same')(conv_0)\n",
        "  maxpool_1 = MaxPool2D(pool_size=(300 - filter_sizes[1] + 1), padding='valid')(conv_1)\n",
        "  maxpool_2 = MaxPool2D(pool_size=(300 - filter_sizes[2] + 1), padding='valid')(conv_2)\n",
        "\n",
        "  concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])\n",
        "  flatten = Flatten()(concatenated_tensor)\n",
        "  dropout = Dropout(drop)(flatten)\n",
        "  output = Dense(units=3, activation='softmax')(dropout)\n",
        "  model = Model(inputs=inputs, outputs=output)\n",
        "  model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[metric])\n",
        "  return model"
      ],
      "metadata": {
        "id": "Mny9HeYjBK8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import *\n",
        "from keras.models import *\n",
        "from keras import backend as K\n",
        "\n",
        "class attention(Layer):\n",
        "    \n",
        "    def __init__(self, return_sequences=True):\n",
        "        self.return_sequences = return_sequences\n",
        "        super(attention,self).__init__()\n",
        "        \n",
        "    def build(self, input_shape):\n",
        "        \n",
        "        self.W=self.add_weight(name=\"att_weight\", shape=(input_shape[-1],1),\n",
        "                               initializer=\"normal\")\n",
        "        self.b=self.add_weight(name=\"att_bias\", shape=(input_shape[1],1),\n",
        "                               initializer=\"zeros\")\n",
        "        \n",
        "        super(attention,self).build(input_shape)\n",
        "        \n",
        "    def call(self, x):\n",
        "        e = K.tanh(K.dot(x,self.W)+self.b)\n",
        "        a = K.softmax(e, axis=1)\n",
        "        output = x*a\n",
        "        \n",
        "        if self.return_sequences:\n",
        "            return output\n",
        "        \n",
        "        return K.sum(output, axis=1)\n",
        "\n",
        "def lstm(size_vector=768):\n",
        "  model = Sequential()\n",
        "  model.add(Input(shape=(size_vector,1), dtype='float32'))\n",
        "  model.add(Bidirectional(LSTM(units=64, return_sequences=True)))\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(3, activation='softmax'))\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy']) \n",
        "  return model\n",
        "\n",
        "def lstm_attention(size_vector=768, metric='accuracy'):\n",
        "  model = Sequential()\n",
        "  model.add(Input(shape=(size_vector,1), dtype='float32'))\n",
        "  model.add(Bidirectional(LSTM(units=64, return_sequences=True)))\n",
        "  model.add(attention()) \n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(3, activation='softmax'))\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy']) \n",
        "  return model"
      ],
      "metadata": {
        "id": "g2WkbOQEB5O3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_bert = pd.read_csv(\"/content/drive/MyDrive/dataset_watson/train_embedded_bert.csv\")\n",
        "x_train, x_test, y_train, y_test = prepare_data_bert(data)"
      ],
      "metadata": {
        "id": "Cl_PhdEJ_JTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_plot(x_train, y_train, model, model_name, metric=\"accuracy\", batch_size = 64, epochs = 200):\n",
        "    history = model.fit(x_train, y_train, validation_split=0.15, batch_size=batch_size, epochs=epochs, verbose=True)\n",
        "    # list all data in history\n",
        "    print(history.history.keys())\n",
        "\n",
        "    # summarize history for accuracy\n",
        "    plt.plot(history.history[metric.lower()], c=\"blue\", label=\"train\")\n",
        "    plt.plot(history.history['val_'+metric.lower()], c=\"orange\", label=\"test\")\n",
        "    plt.title('model '+metric)\n",
        "    plt.ylabel(metric)\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(loc='upper left')\n",
        "    filename = model_name + '_metric_Bert.jpg'\n",
        "    fig.savefig(filename, bbox_inches='tight', dpi=150)\n",
        "    plt.show()\n",
        "    \n",
        "\n",
        "    # summarize history for loss\n",
        "    plt.plot(history.history['loss'], c=\"blue\", label=\"train\")\n",
        "    plt.plot(history.history['val_loss'], c=\"orange\", label=\"test\")\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc='upper left')\n",
        "    filename = model_name + '_loss_Bert.jpg'\n",
        "    fig.savefig(filename, bbox_inches='tight', dpi=150)\n",
        "    plt.show()\n",
        "\n",
        "    return model, history"
      ],
      "metadata": {
        "id": "x6gWqPs7C0mw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CNN_1D_acc = cnn_1('accuracy')\n",
        "model_cnn1_acc,history_cnn1_acc = train_plot(x_train=x_train,\n",
        "                           y_train=y_train,\n",
        "                           model=CNN_1D_acc,\n",
        "                           model_name='CNN_1D_acc'\n",
        "                           metric='accuracy',\n",
        "                           batch_size=64,\n",
        "                           epochs=100)\n",
        "\n",
        "CNN_1D_auc = cnn_1('AUC')\n",
        "model_cnn1_auc,history_cnn1_auc = train_plot(x_train=x_train,\n",
        "                           y_train=y_train,\n",
        "                           model=CNN_1D_auc,\n",
        "                           model_name='CNN_1D_auc'\n",
        "                           metric='AUC',\n",
        "                           batch_size=64,\n",
        "                           epochs=100)"
      ],
      "metadata": {
        "id": "yr6atXvvBtN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CNN_2D_acc = cnn_2('accuracy')\n",
        "model_cnn2_acc,history_cnn2_acc = train_plot(x_train=x_train,\n",
        "                           y_train=y_train,\n",
        "                           model=CNN_2D_acc,\n",
        "                           model_name='CNN_2D_acc'\n",
        "                           metric='accuracy',\n",
        "                           batch_size=64,\n",
        "                           epochs=100)\n",
        "\n",
        "CNN_2D_auc = cnn_2('AUC')\n",
        "model_cnn2_auc,history_cnn2_auc = train_plot(x_train=x_train,\n",
        "                           y_train=y_train,\n",
        "                           model=CNN_2D_auc,\n",
        "                           model_name='CNN_2D_auc'\n",
        "                           metric='AUC',\n",
        "                           batch_size=64,\n",
        "                           epochs=100)"
      ],
      "metadata": {
        "id": "flXF1z83BtrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_acc = lstm('accuracy')\n",
        "model_cnn2_acc,history_cnn2_acc = train_plot(x_train=x_train,\n",
        "                           y_train=y_train,\n",
        "                           model=lstm_acc,\n",
        "                           model_name='lstm_acc'\n",
        "                           metric='accuracy',\n",
        "                           batch_size=64,\n",
        "                           epochs=100)\n",
        "\n",
        "lstm_auc = lstm('AUC')\n",
        "model_cnn2_auc,history_cnn2_auc = train_plot(x_train=x_train,\n",
        "                           y_train=y_train,\n",
        "                           model=lstm_auc,\n",
        "                           model_name='lstm_auc'\n",
        "                           metric='AUC',\n",
        "                           batch_size=64,\n",
        "                           epochs=100)"
      ],
      "metadata": {
        "id": "rFZtDJZLCvdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_att_acc = lstm('accuracy')\n",
        "model_cnn2_acc,history_cnn2_acc = train_plot(x_train=x_train,\n",
        "                           y_train=y_train,\n",
        "                           model=lstm_att_acc,\n",
        "                           model_name='lstm_att_acc'\n",
        "                           metric='accuracy',\n",
        "                           batch_size=64,\n",
        "                           epochs=100)\n",
        "\n",
        "lstm_att_auc = lstm('AUC')\n",
        "model_cnn2_auc,history_cnn2_auc = train_plot(x_train=x_train,\n",
        "                           y_train=y_train,\n",
        "                           model=lstm_att_auc,\n",
        "                           model_name='lstm_att_auc'\n",
        "                           metric='AUC',\n",
        "                           batch_size=64,\n",
        "                           epochs=100)"
      ],
      "metadata": {
        "id": "qQBu85dHCv_d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}