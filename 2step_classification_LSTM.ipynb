{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2step_classification_LSTM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Q_RAp8HvHhOu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
        "from keras.models import Model\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM,Dense,Dropout,Embedding,CuDNNLSTM,Bidirectional, Reshape, Conv2D, MaxPool2D, Concatenate, Flatten, MaxPool1D\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to access data on Google Drive from colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHHUQHXsHoE6",
        "outputId": "c7a153b3-748e-46ff-ad08-cb9110fff2e7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "7chMJqjMyGUY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prep_data(data):\n",
        "  data = data[data['language']=='English']\n",
        "  del data['language']\n",
        "  del data['id']\n",
        "  del data['lang_abv']\n",
        "  data['prem_hyp'] = data[['premise', 'hypothesis']].apply(lambda x: ' [SEP] '.join(x), axis=1)\n",
        "\n",
        "  data['label_0'] = np.where(data['label']==0, 1, 0)\n",
        "  data['label_1'] = np.where(data['label']==1, 1, 0)\n",
        "  data['label_2'] = np.where(data['label']==2, 1, 0)\n",
        "  \n",
        "  # sentences are related if label = 0 (entailment) or 2 (contradiction)\n",
        "  data['related'] = np.where((data['label']==0) | (data['label']==2), 1, 0)\n",
        "  data['related_0'] = np.where((data['related']==0), 1, 0)\n",
        "  data['related_1'] = np.where((data['related']==1), 1, 0)\n",
        "\n",
        "  \n",
        "  data = data.sample(frac=1)\n",
        "  train_size = int(0.7 * len(data))\n",
        "  train_set = data[:train_size]\n",
        "  test_set = data[train_size:]\n",
        "  \n",
        "  return train_set, test_set\n",
        "\n",
        "\n",
        "def tokenize_x(x_train, x_test, size_vector):\n",
        "  token = Tokenizer()\n",
        "  premise_hypothesis = pd.concat([x_train['prem_hyp'], x_test['prem_hyp']], axis=0).values\n",
        "  token.fit_on_texts(premise_hypothesis)\n",
        "\n",
        "  premise_hypothesis_train = x_train['prem_hyp'].values\n",
        "  sequences_train = token.texts_to_sequences(premise_hypothesis_train)\n",
        "  padding_train = pad_sequences(sequences_train,maxlen=size_vector)\n",
        "\n",
        "  premise_hypothesis_test = x_test['prem_hyp'].values\n",
        "  sequences_test = token.texts_to_sequences(premise_hypothesis_test)\n",
        "  padding_test = pad_sequences(sequences_test,maxlen=size_vector)\n",
        "\n",
        "  vocabulary_size = len(token.word_index)+1\n",
        "\n",
        "  return token, padding_train, padding_test, vocabulary_size\n",
        "\n",
        "\n",
        "\n",
        "def get_embedding_matrix(token, path_glove, vocabulary_size, embedding_dim = 300):\n",
        "  embedding_vector = {}\n",
        "  \n",
        "  # get dictionnary from GloVe\n",
        "  f = open (os.path.join(path_glove % embedding_dim))\n",
        "  for line in tqdm(f):\n",
        "      value = line.split(' ')\n",
        "      word = value[0]\n",
        "      coefficient = np.array(value[1:],dtype = 'float32')\n",
        "      embedding_vector[word] = coefficient\n",
        "  \n",
        "  embedding_matrix = np.zeros((vocabulary_size,300))\n",
        "  for word,i in tqdm(token.word_index.items()):\n",
        "      embedding_value = embedding_vector.get(word)\n",
        "      if embedding_value is not None:\n",
        "          embedding_matrix[i] = embedding_value\n",
        "  return embedding_matrix\n",
        "\n"
      ],
      "metadata": {
        "id": "RbuuojzDHqC0"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM model"
      ],
      "metadata": {
        "id": "zR4uoSApyTTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lstm(vocabulary_size, embedding_matrix, embedding_dim=300, size_vector=300, metric='accuracy'):\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(vocabulary_size,embedding_dim,weights = [embedding_matrix],input_length=300,trainable = False))\n",
        "  model.add(Bidirectional(LSTM(64)))\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(Dense(2, activation='softmax'))\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[metric])\n",
        "  return model"
      ],
      "metadata": {
        "id": "bM7SlxpVyWVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment : 2 steps LSTM"
      ],
      "metadata": {
        "id": "ItN5fhxLyZJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_glove = \"/content/drive/MyDrive/GloVe/glove.6B.%sd.txt\"\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/dataset_watson/train.csv\")\n",
        "embedding_dim = 300\n",
        "size_vector = 300"
      ],
      "metadata": {
        "id": "IltamllrygTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set, test_set = prep_data(data)\n",
        "\n",
        "\n",
        "token_related, padding_train_related, padding_test_related, vocabulary_size_related = tokenize_x(train_set, test_set, size_vector)\n",
        "\n",
        "embedding_matrix_related = get_embedding_matrix(token_related, path_glove, vocabulary_size_related, embedding_dim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yt_B0cGXI1fr",
        "outputId": "bd836b1a-e2d8-4f60-9769-3955099cc014"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if __name__ == '__main__':\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  del sys.path[0]\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  from ipykernel import kernelapp as app\n",
            "400000it [00:33, 12055.76it/s]\n",
            "100%|██████████| 12901/12901 [00:00<00:00, 389731.53it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_related = lstm(vocabulary_size_related, embedding_matrix_related)\n",
        "possible_labels = ['related_0', 'related_1']\n",
        "targets_related_train = train_set[possible_labels].values\n",
        "\n",
        "history_related = model_related.fit(padding_train_related,targets_related_train,epochs = 5,batch_size= 64,validation_split=0.2)\n",
        "model_related.summary()\n",
        "\n",
        "# get predictions for related\n",
        "predictions_related=model_related.predict(padding_test_related) \n",
        "predictions_related=np.argmax(predictions_related,axis=1)\n",
        "test_set['prediction_related'] = predictions_related\n",
        "accuracy_related = accuracy_score(test_set['related'], predictions_related)\n",
        "\n",
        "# get index where predictions are 1 (related)\n",
        "index_related_values = np.where(predictions_related==1)\n",
        "\n",
        "# extract subdataset for labels = 1\n",
        "subdataset_related_sentences_train = train_set[train_set['related']==1]\n",
        "subdataset_related_sentences_test = test_set.iloc[index_related_values]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3EhEOcQIFGZ",
        "outputId": "327cb89b-9d8a-4d63-ea61-eb0a1b0a78d0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "61/61 [==============================] - 54s 788ms/step - loss: 0.6288 - accuracy: 0.6813 - val_loss: 0.6170 - val_accuracy: 0.6819\n",
            "Epoch 2/5\n",
            "61/61 [==============================] - 48s 795ms/step - loss: 0.6010 - accuracy: 0.6873 - val_loss: 0.6155 - val_accuracy: 0.6809\n",
            "Epoch 3/5\n",
            "61/61 [==============================] - 46s 757ms/step - loss: 0.5845 - accuracy: 0.6938 - val_loss: 0.6107 - val_accuracy: 0.6798\n",
            "Epoch 4/5\n",
            "61/61 [==============================] - 46s 751ms/step - loss: 0.5653 - accuracy: 0.6995 - val_loss: 0.6137 - val_accuracy: 0.6809\n",
            "Epoch 5/5\n",
            "61/61 [==============================] - 46s 758ms/step - loss: 0.5466 - accuracy: 0.7237 - val_loss: 0.6119 - val_accuracy: 0.6954\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (None, 300, 300)          3870600   \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirectio  (None, 128)              186880    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,057,738\n",
            "Trainable params: 187,138\n",
            "Non-trainable params: 3,870,600\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(accuracy_related)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BScRDlOyOApP",
        "outputId": "984c9069-39af-4c27-dbb7-4dc7ba06aad8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6787967006307618\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# classification for entailment or contradictory\n",
        "\n",
        "token_entailment, padding_train_entailment, padding_test_entailment, vocabulary_size_entailment = tokenize_x(subdataset_related_sentences_train, subdataset_related_sentences_test, size_vector)\n",
        "\n",
        "embedding_matrix_entailment = get_embedding_matrix(token_entailment, path_glove, vocabulary_size_entailment, embedding_dim)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1XjSjGqI6y_",
        "outputId": "656c4ca2-244b-47e9-8720-b53e491597c1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "400000it [00:36, 11014.81it/s]\n",
            "100%|██████████| 11883/11883 [00:00<00:00, 351896.88it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_entailment =  lstm(vocabulary_size_entailment,embedding_matrix_entailment)\n",
        "possible_labels = ['label_0', 'label_2']\n",
        "targets_classification_train = subdataset_related_sentences_train[possible_labels].values\n",
        "history_entailment = model_entailment.fit(padding_train_entailment,targets_classification_train,epochs = 5,batch_size= 64,validation_split=0.2)\n",
        "model_entailment.summary()\n",
        "\n",
        "predictions_entailment = model_entailment.predict(padding_test_entailment) \n",
        "predictions_entailment = np.argmax(predictions_entailment,axis=1)\n",
        "accuracy_entailment = accuracy_score(subdataset_related_sentences_test['label'], predictions_entailment)\n",
        "\n",
        "# In original dataset, replace values by 0, 1 and 2\n",
        "dictionary_index_predictions = dict(zip(index_related_values[0].tolist(), predictions_entailment))\n",
        "\n",
        "# replace all 0 in original dataset (not related) by 1 (neutral)\n",
        "test_set['predictions'] = np.where(test_set['prediction_related']==0, 1, 0)\n",
        "for key in dictionary_index_predictions.keys():\n",
        "  test_set['predictions'][key] = dictionary_index_predictions[key]\n",
        "\n",
        "# get final accuracy\n",
        "accuracy= accuracy_score(test_set['label'], test_set['predictions'])\n",
        "\n",
        "print(\"Accuracy related\",accuracy_related,\"Accuracy entailment\", \n",
        "      accuracy_entailment, \"Final accuracy\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdU5teF4Q1bH",
        "outputId": "24a12e38-46d9-4695-e9d0-a659016db052"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "42/42 [==============================] - 38s 788ms/step - loss: 0.6951 - accuracy: 0.5274 - val_loss: 0.6961 - val_accuracy: 0.5304\n",
            "Epoch 2/5\n",
            "42/42 [==============================] - 32s 776ms/step - loss: 0.6698 - accuracy: 0.5765 - val_loss: 0.6970 - val_accuracy: 0.5486\n",
            "Epoch 3/5\n",
            "42/42 [==============================] - 32s 768ms/step - loss: 0.6468 - accuracy: 0.6313 - val_loss: 0.7028 - val_accuracy: 0.5684\n",
            "Epoch 4/5\n",
            "42/42 [==============================] - 32s 752ms/step - loss: 0.6203 - accuracy: 0.6629 - val_loss: 0.6979 - val_accuracy: 0.6018\n",
            "Epoch 5/5\n",
            "42/42 [==============================] - 32s 769ms/step - loss: 0.5948 - accuracy: 0.6800 - val_loss: 0.7284 - val_accuracy: 0.5684\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_6 (Embedding)     (None, 300, 300)          3565200   \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirectio  (None, 128)              186880    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,752,338\n",
            "Trainable params: 187,138\n",
            "Non-trainable params: 3,565,200\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy related 0.6787967006307618 Accuracy entailment 0.3525852585258526 Final accuracy 0.36147501213003397\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classify related sentence only"
      ],
      "metadata": {
        "id": "FHxAjpdkSK1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_glove = \"/content/drive/MyDrive/GloVe/glove.6B.%sd.txt\"\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/dataset_watson/train.csv\")\n",
        "embedding_dim = 300\n",
        "size_vector = 300"
      ],
      "metadata": {
        "id": "ZMwX6VjRyolf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prep_data_related(data):\n",
        "  data = data[data['language']=='English']\n",
        "  del data['language']\n",
        "  del data['id']\n",
        "  del data['lang_abv']\n",
        "  data['prem_hyp'] = data[['premise', 'hypothesis']].apply(lambda x: ' [SEP] '.join(x), axis=1)\n",
        "  \n",
        "  # sentences are related if label = 0 (entailment) or 2 (contradiction)\n",
        "  data['related'] = np.where((data['label']==0) | (data['label']==2), 1, 0)\n",
        "  data = data[data['related']==1]\n",
        "  del data['related']\n",
        "\n",
        "  data['class0'] = np.where((data['label']==0), 1, 0)\n",
        "  data['class2'] = np.where((data['label']==2), 1, 0)\n",
        "\n",
        "  data = data.sample(frac=1)\n",
        "  train_size = int(0.7 * len(data))\n",
        "  train_set = data[:train_size]\n",
        "  test_set = data[train_size:]\n",
        "  \n",
        "  return train_set, test_set\n",
        "\n",
        "path_glove = \"/content/drive/MyDrive/GloVe/glove.6B.%sd.txt\"\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/dataset_watson/train.csv\")\n",
        "embedding_dim = 300\n",
        "\n"
      ],
      "metadata": {
        "id": "1ML17yojSKNQ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set, test_set = prep_data_related(data)\n",
        "\n",
        "token, padding_train, padding_test, vocabulary_size = tokenize_x(train_set, test_set, size_vector)\n",
        "\n",
        "embedding_matrix= get_embedding_matrix(token, path_glove, vocabulary_size, embedding_dim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cl9cL4tnSUfB",
        "outputId": "5298a553-c8a5-4b9f-e1a4-93a627bd858c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if __name__ == '__main__':\n",
            "400000it [00:31, 12759.55it/s]\n",
            "100%|██████████| 11675/11675 [00:00<00:00, 413264.18it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = lstm(vocabulary_size,embedding_matrix)\n",
        "possible_labels = ['class0', 'class2']\n",
        "targets_train = train_set[possible_labels].values\n",
        "history = model.fit(padding_train,targets_train,epochs = 10,batch_size=256,validation_split=0.2)\n",
        "model.summary()\n",
        "\n",
        "predictions = model.predict(padding_test) \n",
        "predictions= np.argmax(predictions,axis=1)\n",
        "accuracy = accuracy_score(test_set['label'], predictions)\n",
        "\n",
        "print(accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pb1uPii7SUyD",
        "outputId": "f56eae99-7ad5-434c-f6ff-092a750f8d8a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "11/11 [==============================] - 27s 2s/step - loss: 0.6991 - accuracy: 0.5169 - val_loss: 0.6900 - val_accuracy: 0.5296\n",
            "Epoch 2/10\n",
            "11/11 [==============================] - 24s 2s/step - loss: 0.6820 - accuracy: 0.5663 - val_loss: 0.6849 - val_accuracy: 0.5524\n",
            "Epoch 3/10\n",
            "11/11 [==============================] - 23s 2s/step - loss: 0.6676 - accuracy: 0.5902 - val_loss: 0.6776 - val_accuracy: 0.5979\n",
            "Epoch 4/10\n",
            "11/11 [==============================] - 23s 2s/step - loss: 0.6529 - accuracy: 0.6183 - val_loss: 0.6680 - val_accuracy: 0.5979\n",
            "Epoch 5/10\n",
            "11/11 [==============================] - 23s 2s/step - loss: 0.6350 - accuracy: 0.6438 - val_loss: 0.6625 - val_accuracy: 0.6009\n",
            "Epoch 6/10\n",
            "11/11 [==============================] - 24s 2s/step - loss: 0.6169 - accuracy: 0.6589 - val_loss: 0.6579 - val_accuracy: 0.6039\n",
            "Epoch 7/10\n",
            "11/11 [==============================] - 24s 2s/step - loss: 0.5984 - accuracy: 0.6844 - val_loss: 0.6647 - val_accuracy: 0.6024\n",
            "Epoch 8/10\n",
            "11/11 [==============================] - 24s 2s/step - loss: 0.5754 - accuracy: 0.6958 - val_loss: 0.7045 - val_accuracy: 0.5857\n",
            "Epoch 9/10\n",
            "11/11 [==============================] - 24s 2s/step - loss: 0.5629 - accuracy: 0.7129 - val_loss: 0.6934 - val_accuracy: 0.6024\n",
            "Epoch 10/10\n",
            "11/11 [==============================] - 24s 2s/step - loss: 0.5336 - accuracy: 0.7376 - val_loss: 0.7259 - val_accuracy: 0.5903\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_7 (Embedding)     (None, 300, 300)          3502800   \n",
            "                                                                 \n",
            " bidirectional_4 (Bidirectio  (None, 128)              186880    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,689,938\n",
            "Trainable params: 187,138\n",
            "Non-trainable params: 3,502,800\n",
            "_________________________________________________________________\n",
            "0.40509915014164305\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(confusion_matrix(test_set['label'], predictions).ravel())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhq8u8B7UTpe",
        "outputId": "8442bfb2-f7f2-4c68-ae0a-f5b5d563820d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[572 164   0   0   0   0 398 278   0]\n"
          ]
        }
      ]
    }
  ]
}